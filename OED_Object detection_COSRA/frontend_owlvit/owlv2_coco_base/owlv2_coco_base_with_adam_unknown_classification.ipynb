{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b9ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/researchfiles/ECE IMAPLE/cluster_data/user_data/jw3897/research/git/zero-shot-object-detection/frontend_owlvit/owlv2_coco_base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/researchfiles/ECE IMAPLE/cluster_data/user_data/jw3897/research/git/zero-shot-object-detection/frontend_owlvit/owlv2_coco_base'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ~/user_data/research/git/zero-shot-object-detection/frontend_owlvit/owlv2_coco_base\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3984b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16051380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "val_image_dir = r'/mnt/active_storage/Joe/coco_set/val2017/images'\n",
    "val_annotation_path = r'/mnt/active_storage/Joe/coco_set/val2017/annotations/ovd_ins_val2017_b.json'\n",
    "\n",
    "coco_val_dataset = CocoDetection(\n",
    "    root=val_image_dir, \n",
    "    annFile=val_annotation_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbef79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovd_base_image_ids = []\n",
    "for item in coco_val_dataset:\n",
    "    ovd_base_image_ids.append(item[1][0]['image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8522d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['person', 'bicycle', 'car', 'motorcycle', 'train', 'truck', \\\n",
    "    'boat', 'bench', 'bird', 'horse', 'sheep', 'bear', 'zebra', 'giraffe', \\\n",
    "    'backpack', 'handbag', 'suitcase', 'frisbee', 'skis', 'kite', 'surfboard', \\\n",
    "    'bottle', 'fork', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', \\\n",
    "    'broccoli', 'carrot', 'pizza', 'donut', 'chair', 'bed', 'toilet', 'tv', \\\n",
    "    'laptop', 'mouse', 'remote', 'microwave', 'oven', 'toaster', \\\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18573b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_total = pd.read_parquet(r'/mnt/archive/owlvit_results/owlv2_coco_base_predictions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3daa5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>objectness_score</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>139</td>\n",
       "      <td>vase</td>\n",
       "      <td>0.351422</td>\n",
       "      <td>0.227661</td>\n",
       "      <td>350.876923</td>\n",
       "      <td>209.574448</td>\n",
       "      <td>362.114807</td>\n",
       "      <td>230.386261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>139</td>\n",
       "      <td>vase</td>\n",
       "      <td>0.334456</td>\n",
       "      <td>0.284153</td>\n",
       "      <td>166.681076</td>\n",
       "      <td>232.826904</td>\n",
       "      <td>186.017776</td>\n",
       "      <td>266.831390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>139</td>\n",
       "      <td>tv</td>\n",
       "      <td>0.704314</td>\n",
       "      <td>0.542719</td>\n",
       "      <td>5.215874</td>\n",
       "      <td>166.488068</td>\n",
       "      <td>154.236847</td>\n",
       "      <td>267.109009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>139</td>\n",
       "      <td>vase</td>\n",
       "      <td>0.248859</td>\n",
       "      <td>0.221027</td>\n",
       "      <td>240.760803</td>\n",
       "      <td>197.673264</td>\n",
       "      <td>253.225784</td>\n",
       "      <td>213.093719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>139</td>\n",
       "      <td>tv</td>\n",
       "      <td>0.409905</td>\n",
       "      <td>0.306233</td>\n",
       "      <td>559.908569</td>\n",
       "      <td>209.109192</td>\n",
       "      <td>640.466003</td>\n",
       "      <td>289.297943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178952</th>\n",
       "      <td>581781</td>\n",
       "      <td>orange</td>\n",
       "      <td>0.176230</td>\n",
       "      <td>0.327590</td>\n",
       "      <td>281.273376</td>\n",
       "      <td>434.908295</td>\n",
       "      <td>371.029541</td>\n",
       "      <td>478.781433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178161</th>\n",
       "      <td>581781</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.284449</td>\n",
       "      <td>0.241578</td>\n",
       "      <td>201.138290</td>\n",
       "      <td>274.905090</td>\n",
       "      <td>270.746216</td>\n",
       "      <td>331.941162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177593</th>\n",
       "      <td>581781</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.301017</td>\n",
       "      <td>0.333404</td>\n",
       "      <td>445.379303</td>\n",
       "      <td>101.757217</td>\n",
       "      <td>638.398376</td>\n",
       "      <td>269.523865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178102</th>\n",
       "      <td>581781</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.323884</td>\n",
       "      <td>0.339627</td>\n",
       "      <td>141.406525</td>\n",
       "      <td>184.186020</td>\n",
       "      <td>441.437103</td>\n",
       "      <td>460.155182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178986</th>\n",
       "      <td>581781</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.386758</td>\n",
       "      <td>0.336109</td>\n",
       "      <td>52.484303</td>\n",
       "      <td>453.187256</td>\n",
       "      <td>100.978386</td>\n",
       "      <td>477.885498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33571 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_id   label     score  objectness_score        xmin        ymin  \\\n",
       "1233         139    vase  0.351422          0.227661  350.876923  209.574448   \n",
       "1336         139    vase  0.334456          0.284153  166.681076  232.826904   \n",
       "1150         139      tv  0.704314          0.542719    5.215874  166.488068   \n",
       "1163         139    vase  0.248859          0.221027  240.760803  197.673264   \n",
       "1253         139      tv  0.409905          0.306233  559.908569  209.109192   \n",
       "...          ...     ...       ...               ...         ...         ...   \n",
       "178952    581781  orange  0.176230          0.327590  281.273376  434.908295   \n",
       "178161    581781  banana  0.284449          0.241578  201.138290  274.905090   \n",
       "177593    581781  banana  0.301017          0.333404  445.379303  101.757217   \n",
       "178102    581781  banana  0.323884          0.339627  141.406525  184.186020   \n",
       "178986    581781  banana  0.386758          0.336109   52.484303  453.187256   \n",
       "\n",
       "              xmax        ymax  \n",
       "1233    362.114807  230.386261  \n",
       "1336    186.017776  266.831390  \n",
       "1150    154.236847  267.109009  \n",
       "1163    253.225784  213.093719  \n",
       "1253    640.466003  289.297943  \n",
       "...            ...         ...  \n",
       "178952  371.029541  478.781433  \n",
       "178161  270.746216  331.941162  \n",
       "177593  638.398376  269.523865  \n",
       "178102  441.437103  460.155182  \n",
       "178986  100.978386  477.885498  \n",
       "\n",
       "[33571 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objectness_threshold = 0.2\n",
    "score_threshold = 0.1\n",
    "\n",
    "predictions_likely_objects = predictions_df_total[predictions_df_total['objectness_score'] >= objectness_threshold].sort_values('objectness_score', ascending=False).groupby('image_id').head(100).sort_values('image_id')\n",
    "predictions_likely_objects = predictions_likely_objects.loc[predictions_likely_objects['image_id'].isin(ovd_base_image_ids)]\n",
    "\n",
    "known_objects = predictions_likely_objects[predictions_likely_objects['score'] >= score_threshold]\n",
    "\n",
    "display(known_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e7404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_image_ids = known_objects['image_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d050df",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_classifications = torch.load(r'./output_with_scores.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97784924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 139, 'boxes': tensor([[350.8769, 209.5744, 362.1148, 230.3863],\n",
      "        [166.6811, 232.8269, 186.0178, 266.8314],\n",
      "        [  5.2159, 166.4881, 154.2368, 267.1090],\n",
      "        [240.7608, 197.6733, 253.2258, 213.0937],\n",
      "        [559.9086, 209.1092, 640.4660, 289.2979],\n",
      "        [549.7681, 306.8633, 586.3107, 400.1674],\n",
      "        [555.1292, 290.8761, 577.7758, 331.9653],\n",
      "        [466.0369, 354.4450, 637.8002, 425.3321],\n",
      "        [337.8965, 176.3780, 381.8003, 222.0279],\n",
      "        [  2.1859,   2.0000,   2.3986,   1.0000],\n",
      "        [505.3746,  84.2601, 534.2266, 135.9551],\n",
      "        [120.8437, 356.6360, 466.8375, 426.6800],\n",
      "        [573.5719, 249.9596, 606.8554, 309.2519]]), 'labels': tensor([33, 33, 43, 43, 43, 43, 30, 45, 43, 32, 21, 46, 47], dtype=torch.int32), 'scores': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.3367, 0.9678, 0.4067,\n",
      "        0.6875, 0.4932, 0.9614, 0.4680])}\n"
     ]
    }
   ],
   "source": [
    "print(adam_classifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb38ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_image_ids = [adam_classification['image_id'] for adam_classification in adam_classifications]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e858eab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(adam_classifications[0]['image_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f0a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_only_ids = list(set(known_image_ids) - set(adam_image_ids))\n",
    "for idx, id in enumerate(known_only_ids):\n",
    "    known_only_ids[idx] = int(known_only_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c35363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n"
     ]
    }
   ],
   "source": [
    "print(len(known_only_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f9027df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3892\n"
     ]
    }
   ],
   "source": [
    "print(len(adam_classifications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded7c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_classifications = adam_classifications.copy()\n",
    "\n",
    "for known_only_id in known_only_ids:\n",
    "    combined_classifications.append({\n",
    "        'image_id': int(known_only_id),\n",
    "        'boxes': [],\n",
    "        'labels': [],\n",
    "        'scores': []\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aed24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_idx, row in known_objects.iterrows():\n",
    "    # Get image_id of this annotation\n",
    "    row_image_id = int(row['image_id'])\n",
    "    # iterate through all the classifications in the list so we can index correctly\n",
    "    for img_idx, image_data in enumerate(combined_classifications):\n",
    "        if  image_data['image_id'] == row_image_id:\n",
    "            for score_idx, score in enumerate(combined_classifications[img_idx]['scores']):\n",
    "                if (score == 1.0 \n",
    "                and float(f\"{row['xmin']:.4f}\") == float(f\"{combined_classifications[img_idx]['boxes'][score_idx][0]:.4f}\") \n",
    "                and float(f\"{row['ymin']:.4f}\") == float(f\"{combined_classifications[img_idx]['boxes'][score_idx][1]:.4f}\")\n",
    "                and float(f\"{row['xmax']:.4f}\") == float(f\"{combined_classifications[img_idx]['boxes'][score_idx][2]:.4f}\")\n",
    "                and float(f\"{row['ymax']:.4f}\") == float(f\"{combined_classifications[img_idx]['boxes'][score_idx][3]:.4f}\")):\n",
    "                    combined_classifications[img_idx]['scores'][score_idx] = row['score']\n",
    "            if row_image_id in known_only_ids:\n",
    "                combined_classifications[img_idx]['boxes'].append([row['xmin'], row['ymin'], row['xmax'], row['ymax']])\n",
    "                combined_classifications[img_idx]['labels'].append(category_names.index(row['label']))\n",
    "                combined_classifications[img_idx]['scores'].append(row['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6752678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, classification in enumerate(combined_classifications):\n",
    "    if type(classification['boxes']) == type([]):\n",
    "        combined_classifications[idx]['boxes'] = torch.tensor(combined_classifications[idx]['boxes'], dtype=torch.int)\n",
    "        combined_classifications[idx]['labels'] = torch.tensor(combined_classifications[idx]['labels'], dtype=torch.int)\n",
    "        combined_classifications[idx]['scores'] = torch.tensor(combined_classifications[idx]['scores'], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e104ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image_ids = [item['image_id'] for item in combined_classifications]\n",
    "missing_image_ids = list(set(ovd_base_image_ids) - set(predicted_image_ids))\n",
    "\n",
    "for missing_image_id in missing_image_ids:\n",
    "    combined_classifications.append({\n",
    "        'image_id': missing_image_id,\n",
    "        'boxes': torch.tensor([], dtype=torch.int),\n",
    "        'labels': torch.tensor([], dtype=torch.int),\n",
    "        'scores': torch.tensor([], dtype=torch.int)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd36d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(combined_classifications) == len(coco_val_dataset), \"The classifications list is missing images from the ground truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc63a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_write_path = r'/mnt/archive/owlvit_results/coco_base_obj2cls1_combined_prepped_for_torchmetrics.pt'\n",
    "torch.save(combined_classifications, torch_write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2d3c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_torch = torch.load(torch_write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52b07b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(combined_classifications) == len(loaded_torch), \"Saved and loaded lists are different lengths\"\n",
    "\n",
    "for idx in range(len(combined_classifications)):\n",
    "    assert combined_classifications[idx]['image_id'] == loaded_torch[idx]['image_id'], f\"Mismatched image ids at index {idx}\"\n",
    "    assert torch.equal(combined_classifications[idx]['boxes'], loaded_torch[idx]['boxes']), f\"Mismatched boxes at index {idx}\"\n",
    "    assert torch.equal(combined_classifications[idx]['labels'], loaded_torch[idx]['labels']), f\"Mismatched labels at index {idx}\"\n",
    "    assert torch.equal(combined_classifications[idx]['scores'], loaded_torch[idx]['scores']), f\"Mismatched scores at index {idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018bff30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
