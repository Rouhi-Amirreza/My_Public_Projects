{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/user_data/research/git/zero-shot-object-detection/frontend_owlvit/owlv2_clip_coco_base\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "from transformers import pipeline\n",
    "from transformers import Owlv2ImageProcessor, Owlv2Processor, Owlv2ForObjectDetection\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_dir = r'/mnt/active_storage/Joe/coco_set/val2017/images'\n",
    "val_annotation_path = r'/mnt/active_storage/Joe/coco_set/val2017/annotations/ovd_ins_val2017_b.json'\n",
    "\n",
    "coco_val_dataset = CocoDetection(\n",
    "    root=val_image_dir, \n",
    "    annFile=val_annotation_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(coco_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovd_base_image_ids = []\n",
    "for item in coco_val_dataset:\n",
    "    ovd_base_image_ids.append(item[1][0]['image_id'])\n",
    "\n",
    "print(len(ovd_base_image_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_filenames = os.listdir(val_image_dir)\n",
    "# image_filenames.sort()\n",
    "\n",
    "# image_ids = []\n",
    "# for image_filename in image_filenames:\n",
    "#     image_id = image_filename.split('.')[0].lstrip('0')\n",
    "#     image_ids.append(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['person', 'bicycle', 'car', 'motorcycle', 'train', 'truck', \\\n",
    "    'boat', 'bench', 'bird', 'horse', 'sheep', 'bear', 'zebra', 'giraffe', \\\n",
    "    'backpack', 'handbag', 'suitcase', 'frisbee', 'skis', 'kite', 'surfboard', \\\n",
    "    'bottle', 'fork', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', \\\n",
    "    'broccoli', 'carrot', 'pizza', 'donut', 'chair', 'bed', 'toilet', 'tv', \\\n",
    "    'laptop', 'mouse', 'remote', 'microwave', 'oven', 'toaster', \\\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids = coco_val_dataset.coco.loadCats(coco_val_dataset.coco.getCatIds())\n",
    "\n",
    "targets = []\n",
    "target_boxes = []\n",
    "target_labels = []\n",
    "\n",
    "for idx, image_tuple in enumerate(coco_val_dataset):\n",
    "    image = image_tuple[0]\n",
    "    annotations = image_tuple[1]\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        label_name = [category['name'] for category in category_ids if category['id'] == annotation['category_id']][0]\n",
    "\n",
    "        try:\n",
    "            label = category_names.index(label_name)\n",
    "        except ValueError as e:\n",
    "            # If the annotation is not part of the base categories, skip it\n",
    "            continue\n",
    "\n",
    "        # xmin, ymin, width, height -> xmin, ymin, xmax, ymax\n",
    "        box = annotation['bbox']\n",
    "        bbox = []\n",
    "        bbox.append(int(round(box[0])))\n",
    "        bbox.append(int(round(box[1])))\n",
    "        bbox.append(int(round(box[0] + box[2])))\n",
    "        bbox.append(int(round(box[1] + box[3])))\n",
    "\n",
    "        target_boxes.append(bbox)\n",
    "        target_labels.append(label)\n",
    "\n",
    "    target_boxes = torch.tensor(target_boxes, dtype=torch.int)\n",
    "    target_labels = torch.tensor(target_labels, dtype=torch.int)\n",
    "\n",
    "    target_image_dict = {\n",
    "        'image_id': ovd_base_image_ids[idx],\n",
    "        'boxes': target_boxes,\n",
    "        'labels': target_labels\n",
    "    }\n",
    "\n",
    "    targets.append(target_image_dict)\n",
    "\n",
    "    # Clear boxes and labels\n",
    "    target_boxes = []\n",
    "    target_labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"openai/clip-vit-base-patch32\"\n",
    "# model = CLIPModel.from_pretrained(model_checkpoint)\n",
    "# processor = AutoProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_inputs = []\n",
    "# for category_name in category_names:\n",
    "#     text_inputs.append(f\"a photo of a {category_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_total = pd.read_parquet(r'./owlv2_coco_base_predictions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectness_threshold = 0.2\n",
    "score_threshold = 0.1\n",
    "\n",
    "predictions_likely_objects = predictions_df_total[predictions_df_total['objectness_score'] >= objectness_threshold].sort_values('objectness_score', ascending=False).groupby('image_id').head(100).sort_values('image_id')\n",
    "predictions_likely_objects = predictions_likely_objects.loc[predictions_likely_objects['image_id'].isin(ovd_base_image_ids)]\n",
    "\n",
    "known_objects = predictions_likely_objects[predictions_likely_objects['score'] >= score_threshold]\n",
    "unknown_objects = predictions_likely_objects[predictions_likely_objects['score'] < score_threshold]\n",
    "\n",
    "display(known_objects)\n",
    "display(unknown_objects)\n",
    "\n",
    "print(f\"Number of Known Annotations: {len(known_objects)}. Number of Unknown Annotations: {len(unknown_objects)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_id | [known object labels] | [known object locations] | unknown object location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_objects_updated = pd.read_parquet(r'./unknown_objects_updated_obj2_cls1_clipvitlarge14.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(unknown_objects_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_objects_updated_filtered = unknown_objects_updated[unknown_objects_updated['score'] >= score_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects = pd.concat([known_objects, unknown_objects_updated_filtered])\n",
    "all_objects = all_objects.sort_values(by='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = all_objects.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "pred_boxes = []\n",
    "pred_scores = []\n",
    "pred_objectness_scores = []\n",
    "pred_labels = []\n",
    "\n",
    "current_img_id = predictions_dict[0]['image_id']\n",
    "for prediction in predictions_dict:\n",
    "    # If on a new image...\n",
    "    if prediction['image_id'] != current_img_id:\n",
    "        # Convert boxes, scores, and labels to tensors\n",
    "        pred_boxes = torch.tensor(pred_boxes, dtype=torch.int)\n",
    "        pred_scores = torch.tensor(pred_scores, dtype=torch.float)\n",
    "        pred_objectness_scores = torch.tensor(pred_objectness_scores, dtype=torch.float)\n",
    "        pred_labels = torch.tensor(pred_labels, dtype=torch.int)\n",
    "\n",
    "        # Create the dict for preds\n",
    "        pred_image_dict = {\n",
    "            'image_id': current_img_id,\n",
    "            'boxes': pred_boxes,\n",
    "            'scores': pred_scores,\n",
    "            'objectness_scores': pred_objectness_scores,\n",
    "            'labels': pred_labels\n",
    "        }\n",
    "\n",
    "        preds.append(pred_image_dict)\n",
    "\n",
    "        # Clear boxes, scores, and labels for the new image\n",
    "        pred_boxes = []\n",
    "        pred_scores = []\n",
    "        pred_objectness_scores = []\n",
    "        pred_labels = []\n",
    "\n",
    "        # Set new current image id\n",
    "        current_img_id = prediction['image_id']\n",
    "\n",
    "    bbox = []\n",
    "    bbox.append(prediction['xmin'])\n",
    "    bbox.append(prediction['ymin'])\n",
    "    bbox.append(prediction['xmax'])\n",
    "    bbox.append(prediction['ymax'])\n",
    "\n",
    "    pred_boxes.append(bbox)\n",
    "    pred_scores.append(prediction['score'])\n",
    "    pred_objectness_scores.append(prediction['objectness_score'])\n",
    "    pred_labels.append(category_names.index(prediction['label']))\n",
    "\n",
    "# Capture the predictions for the last image\n",
    "pred_boxes = torch.tensor(pred_boxes, dtype=torch.int)\n",
    "pred_scores = torch.tensor(pred_scores, dtype=torch.float)\n",
    "pred_objectness_scores = torch.tensor(pred_objectness_scores, dtype=torch.float)\n",
    "pred_labels = torch.tensor(pred_labels, dtype=torch.int)\n",
    "\n",
    "# Create the dict for preds\n",
    "pred_image_dict = {\n",
    "    'image_id': current_img_id,\n",
    "    'boxes': pred_boxes,\n",
    "    'scores': pred_scores,\n",
    "    'objectness_scores': pred_objectness_scores,\n",
    "    'labels': pred_labels\n",
    "}\n",
    "\n",
    "preds.append(pred_image_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(preds) < len(coco_val_dataset):\n",
    "    pred_image_ids = np.unique([pred['image_id'] for pred in preds])\n",
    "    missing_ids = list(set(ovd_base_image_ids) - set(pred_image_ids))\n",
    "    missing_ids.sort()\n",
    "\n",
    "    for missing_id in missing_ids:\n",
    "        missing_id_dict = {\n",
    "            'image_id': missing_id,\n",
    "            'boxes': torch.tensor([], dtype=torch.int),\n",
    "            'scores': torch.tensor([], dtype=torch.float),\n",
    "            'objectness_scores': torch.tensor([], dtype=torch.float),\n",
    "            'labels': torch.tensor([], dtype=torch.int),\n",
    "        }\n",
    "        preds.insert(ovd_base_image_ids.index(missing_id), missing_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(preds) == len(coco_val_dataset)\n",
    "for idx in range (len(preds)):\n",
    "    assert preds[idx]['image_id'] == ovd_base_image_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(targets)):\n",
    "    assert targets[idx]['image_id'] == preds[idx]['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some preds and targets\n",
    "from PIL import ImageDraw\n",
    "\n",
    "index = 0\n",
    "\n",
    "visualized_image = coco_val_dataset[index][0].copy()\n",
    "\n",
    "draw = ImageDraw.Draw(visualized_image)\n",
    "\n",
    "for box, label in zip(preds[index]['boxes'], preds[index]['labels']):\n",
    "    draw.rectangle(xy=((box[0], box[1]), (box[2], box[3])), outline='red')\n",
    "    draw.text(xy=(box[0], box[1]), text=category_names[label.item()])\n",
    "\n",
    "for box, label in zip(targets[index]['boxes'], targets[index]['labels']):\n",
    "    draw.rectangle(xy=((box[0], box[1]), (box[2], box[3])), outline='green')\n",
    "    draw.text(xy=(box[0], box[1]), text=category_names[label.item()])\n",
    "\n",
    "display(visualized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MeanAveragePrecision(\n",
    "    box_format='xyxy',\n",
    "    iou_type='bbox',\n",
    "    iou_thresholds=None, # Defaults to trying from 0.5 -> 0.95 in steps of 0.05 \n",
    "    rec_thresholds=None,\n",
    "    max_detection_thresholds=None, # Uses [1, 10, 100]\n",
    "    class_metrics=False,\n",
    "    extended_summary=False, # This way, we can see the ious and scores calculated\n",
    "    average='macro',\n",
    "    backend='pycocotools'\n",
    ")\n",
    "\n",
    "metric.update(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_metric = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Objectness Threshold: {objectness_threshold}; Score Threshold: {score_threshold}\")\n",
    "print(f\"mAP: {round(computed_metric['map'].item(), 4):.4f}\\nKnown: {len(known_objects)}\\nUnknown: {len(unknown_objects)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectness Threshold: 0.2; Score Threshold: 0.1, using all objects with CLIP to update the unknowns, CLIP filtered at score threshold. \\\n",
    "mAP: 0.2417\n",
    "Known: 27439\n",
    "Unknown: 33519"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
